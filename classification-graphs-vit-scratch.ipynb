{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# mounting google drive where data has been stored\n\n# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"abPSIMtLgh5o","outputId":"816f7b3b-3f60-4836-e767-f83be1effcef","execution":{"iopub.status.busy":"2022-08-07T10:11:39.537427Z","iopub.execute_input":"2022-08-07T10:11:39.538108Z","iopub.status.idle":"2022-08-07T10:11:39.558020Z","shell.execute_reply.started":"2022-08-07T10:11:39.537997Z","shell.execute_reply":"2022-08-07T10:11:39.557112Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# checking whether a file exists\n\nimport os\n# file_path = \"/content/drive/MyDrive/IIT Jodhpur/charts/train_val.csv\"\nfile_path = \"../input/graphdatasetiitjohdpur/chartsv2/charts/train_val.csv\"\nprint(os.path.exists(file_path))","metadata":{"id":"Yue25VKUiEyR","outputId":"e081ceb8-0ba8-4637-e9d6-54bb54d97445","execution":{"iopub.status.busy":"2022-08-07T10:42:53.950385Z","iopub.execute_input":"2022-08-07T10:42:53.951308Z","iopub.status.idle":"2022-08-07T10:42:53.994105Z","shell.execute_reply.started":"2022-08-07T10:42:53.951180Z","shell.execute_reply":"2022-08-07T10:42:53.993084Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# read the csv file into a dataframe\n\nimport pandas as pd\ndf = pd.read_csv(file_path)\ndf.head()","metadata":{"id":"U5d2mzDxiKck","outputId":"36cd3bda-3c0d-49fe-c895-862eadd2c9b5","execution":{"iopub.status.busy":"2022-08-07T10:43:10.613897Z","iopub.execute_input":"2022-08-07T10:43:10.617066Z","iopub.status.idle":"2022-08-07T10:43:10.652440Z","shell.execute_reply.started":"2022-08-07T10:43:10.617008Z","shell.execute_reply":"2022-08-07T10:43:10.648509Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# check for the unique labels in the dataframe\n\ndf.type.unique()","metadata":{"id":"CrVfI-Yqiqqn","outputId":"cacf126d-675a-4fed-b6c3-8dd2fbf5c009","execution":{"iopub.status.busy":"2022-08-07T10:43:34.632926Z","iopub.execute_input":"2022-08-07T10:43:34.633607Z","iopub.status.idle":"2022-08-07T10:43:34.647476Z","shell.execute_reply.started":"2022-08-07T10:43:34.633572Z","shell.execute_reply":"2022-08-07T10:43:34.645974Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# fetching the training data\n\n# train_dir = \"/content/drive/MyDrive/IIT Jodhpur/charts/train_val\"\ntrain_dir = \"../input/graphdatasetiitjohdpur/chartsv2/charts/train_val\"\n\nimport os\ntrain_paths = os.listdir(train_dir)\nprint(len(train_paths))","metadata":{"id":"7xyeyatdjSyf","outputId":"f688702b-ac7a-4332-ef1e-4b85c6debc02","execution":{"iopub.status.busy":"2022-08-07T10:43:38.653857Z","iopub.execute_input":"2022-08-07T10:43:38.654226Z","iopub.status.idle":"2022-08-07T10:43:38.812052Z","shell.execute_reply.started":"2022-08-07T10:43:38.654195Z","shell.execute_reply":"2022-08-07T10:43:38.810928Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# storing the images and the labels into an array\n\nimport numpy as np\nimport cv2\ndef sort_data(dir, data, df):\n    images = []\n    labels = []\n    for i in range(len(data)):\n        img = cv2.imread(os.path.join(dir, data[i]))    # reading the images from the directory\n        index = int(data[i].split(\".\")[0])\n        label = df.loc[index, 'type']                   # fetching the associated labels\n        labels.append(label)\n        images.append(img)\n    return np.array(images), np.array(labels)\n\nX_train, Y_train = sort_data(train_dir, train_paths, df)\n\nprint(X_train.shape)\nprint(Y_train.shape)","metadata":{"id":"SFLEJ7PDli6R","outputId":"20beaf3e-b069-4dd6-f84a-5bc392fc450d","execution":{"iopub.status.busy":"2022-08-07T10:43:42.413837Z","iopub.execute_input":"2022-08-07T10:43:42.414931Z","iopub.status.idle":"2022-08-07T10:43:45.745123Z","shell.execute_reply.started":"2022-08-07T10:43:42.414889Z","shell.execute_reply":"2022-08-07T10:43:45.744065Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Look's like we have 1000 samples so we can go ahead and divide the into 800 for training and 200 for validation","metadata":{"id":"_vFQngWtVk-e"}},{"cell_type":"code","source":"# spliting the dataset into validation and training\n\nfrom sklearn.model_selection import train_test_split\n \nx_train, x_validation, y_train, y_validation = train_test_split(\n    X_train, \n    Y_train, \n    test_size=200, \n    shuffle=True,\n    random_state=42,\n    stratify=Y_train\n)\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_validation.shape)\nprint(y_validation.shape)","metadata":{"id":"aktr-YPJtc1C","outputId":"7eca71bd-5b82-4f16-86a0-d3ed539a86a9","execution":{"iopub.status.busy":"2022-08-07T10:45:01.738436Z","iopub.execute_input":"2022-08-07T10:45:01.738804Z","iopub.status.idle":"2022-08-07T10:45:02.243922Z","shell.execute_reply.started":"2022-08-07T10:45:01.738768Z","shell.execute_reply":"2022-08-07T10:45:02.242879Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# checking the distribution among all the labels\n\nprint(np.unique(y_train, return_counts=True))\nprint(np.unique(y_validation, return_counts=True))","metadata":{"id":"j1A3RW12MWpJ","outputId":"bed5d8f1-592c-4ab3-f75c-0fe165b87943","execution":{"iopub.status.busy":"2022-08-07T10:45:10.055351Z","iopub.execute_input":"2022-08-07T10:45:10.055693Z","iopub.status.idle":"2022-08-07T10:45:10.062893Z","shell.execute_reply.started":"2022-08-07T10:45:10.055665Z","shell.execute_reply":"2022-08-07T10:45:10.061712Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# integer encoding \n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(y_train)\ny_train_enc = le.transform(y_train)\ny_validation_enc = le.transform(y_validation)\n\nprint(np.unique(y_train_enc, return_counts=True))\nprint(np.unique(y_validation_enc, return_counts=True))","metadata":{"id":"mjHQWubT79LX","outputId":"9b2df44b-8f42-47ed-d2fe-35e06b3ff558","execution":{"iopub.status.busy":"2022-08-07T10:45:13.598775Z","iopub.execute_input":"2022-08-07T10:45:13.599241Z","iopub.status.idle":"2022-08-07T10:45:13.607699Z","shell.execute_reply.started":"2022-08-07T10:45:13.599208Z","shell.execute_reply":"2022-08-07T10:45:13.606466Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Ploting random training data to see if the encoding is done properly and also to visualise images","metadata":{"id":"biyxhoURWEw7"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\nn = np.random.randint(0, 800, 6)\nplt.figure(figsize=(20, 4))\nfor i in range(1, len(n)+1): \n    ax = plt.subplot(1, len(n), i)\n    img = x_train[n[i-1]]\n    plt.imshow(img)\n    ax.text(0.7, -0.4, \"label-{0}\\nencoded-{1}\".format(y_train[n[i-1]], y_train_enc[n[i-1]]),\n    verticalalignment='bottom', horizontalalignment='right',\n    transform=ax.transAxes, fontsize=10)\n    plt.axis(\"off\")\nplt.show()","metadata":{"id":"krs4epY7yoDE","outputId":"1d086881-87e6-47b3-881c-9406a7f55bcb","execution":{"iopub.status.busy":"2022-08-07T10:47:04.344520Z","iopub.execute_input":"2022-08-07T10:47:04.344897Z","iopub.status.idle":"2022-08-07T10:47:04.726950Z","shell.execute_reply.started":"2022-08-07T10:47:04.344866Z","shell.execute_reply":"2022-08-07T10:47:04.725888Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(y_train.shape)\nprint(y_validation.shape)","metadata":{"id":"CtTpEACG8ccd","outputId":"47bfbdaf-e787-4003-eb21-43514044c10e","execution":{"iopub.status.busy":"2022-08-07T10:47:24.436462Z","iopub.execute_input":"2022-08-07T10:47:24.436817Z","iopub.status.idle":"2022-08-07T10:47:24.443820Z","shell.execute_reply.started":"2022-08-07T10:47:24.436787Z","shell.execute_reply":"2022-08-07T10:47:24.442878Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# enum class helps when we to the prediction to see the associated label against each predicted integer\n\nfrom enum import Enum\nclass Number(Enum):\n    dot_line = 0\n    hbar = 1\n    line = 2\n    pie = 3\n    vbar = 4","metadata":{"id":"p7aZoHvvNZCC","execution":{"iopub.status.busy":"2022-08-07T10:47:26.731056Z","iopub.execute_input":"2022-08-07T10:47:26.731944Z","iopub.status.idle":"2022-08-07T10:47:26.739662Z","shell.execute_reply.started":"2022-08-07T10:47:26.731910Z","shell.execute_reply":"2022-08-07T10:47:26.736769Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# installing aditional library wherein tensorflow transformer implementation is there\n!pip install -U tensorflow-addons","metadata":{"id":"RZR71vexskXh","outputId":"ba8c8a96-a58f-4675-ff22-3d55be8975a0","execution":{"iopub.status.busy":"2022-08-07T10:47:29.033384Z","iopub.execute_input":"2022-08-07T10:47:29.033747Z","iopub.status.idle":"2022-08-07T10:47:49.371199Z","shell.execute_reply.started":"2022-08-07T10:47:29.033718Z","shell.execute_reply":"2022-08-07T10:47:49.370032Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# MODEL parameters are initialised in one section\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\n\ninput_shape = (128, 128, 3)\nnum_classes = 5\n\nlearning_rate = 0.0001\nweight_decay = 0.0001\nbatch_size = 64\nnum_epochs = 30\nimage_size = 128  # We'll resize input images to this size\npatch_size = 16  # Size of the patches to be extract from the input images\nnum_patches = (image_size // patch_size) ** 2\nprojection_dim = 512\nnum_heads = 4\ntransformer_units = [\n    projection_dim * 2,\n    projection_dim,\n]  # Size of the transformer layers\ntransformer_layers = 8\nmlp_head_units = [2048, 1024]","metadata":{"id":"7Iz1t4TQ8l5Y","execution":{"iopub.status.busy":"2022-08-07T10:56:34.788797Z","iopub.execute_input":"2022-08-07T10:56:34.789192Z","iopub.status.idle":"2022-08-07T10:56:34.796910Z","shell.execute_reply.started":"2022-08-07T10:56:34.789159Z","shell.execute_reply":"2022-08-07T10:56:34.795696Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-08-07T10:48:45.093290Z","iopub.execute_input":"2022-08-07T10:48:45.094188Z","iopub.status.idle":"2022-08-07T10:48:45.098409Z","shell.execute_reply.started":"2022-08-07T10:48:45.094151Z","shell.execute_reply":"2022-08-07T10:48:45.097379Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"data_augmentation = keras.Sequential(\n    [\n        layers.Normalization(),\n        layers.Resizing(image_size, image_size)\n    ],\n    name=\"data_augmentation\",\n)\n# Compute the mean and the variance of the training data for normalization.\ndata_augmentation.layers[0].adapt(x_train)","metadata":{"id":"fZK8hpDXswwj","execution":{"iopub.status.busy":"2022-08-07T10:48:49.776375Z","iopub.execute_input":"2022-08-07T10:48:49.777081Z","iopub.status.idle":"2022-08-07T10:48:53.117026Z","shell.execute_reply.started":"2022-08-07T10:48:49.777020Z","shell.execute_reply":"2022-08-07T10:48:53.116004Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Implementing MLP\ndef mlp(x, hidden_units, dropout_rate):\n    for units in hidden_units:\n        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n        x = layers.Dropout(dropout_rate)(x)\n    return x","metadata":{"id":"PPYVqRBBs7zC","execution":{"iopub.status.busy":"2022-08-07T10:49:50.779659Z","iopub.execute_input":"2022-08-07T10:49:50.780429Z","iopub.status.idle":"2022-08-07T10:49:50.786510Z","shell.execute_reply.started":"2022-08-07T10:49:50.780386Z","shell.execute_reply":"2022-08-07T10:49:50.785339Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Patch creation Layer\nclass Patches(layers.Layer):\n    def __init__(self, patch_size):\n        super(Patches, self).__init__()\n        self.patch_size = patch_size\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches","metadata":{"id":"ZnPb4XovtBVi","execution":{"iopub.status.busy":"2022-08-07T10:49:54.485574Z","iopub.execute_input":"2022-08-07T10:49:54.485929Z","iopub.status.idle":"2022-08-07T10:49:54.493357Z","shell.execute_reply.started":"2022-08-07T10:49:54.485898Z","shell.execute_reply":"2022-08-07T10:49:54.491643Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Display the patches\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(4, 4))\nimage = x_train[np.random.choice(range(x_train.shape[0]))]\nplt.imshow(image/255)\nplt.axis(\"off\")\n\nresized_image = tf.image.resize(\n    tf.convert_to_tensor([image]), size=(image_size, image_size)\n)\n\n\npatches = Patches(patch_size)(resized_image)\nprint(f\"Image size: {image_size} X {image_size}\")\nprint(f\"Patch size: {patch_size} X {patch_size}\")\nprint(f\"Patches per image: {patches.shape[1]}\")\nprint(f\"Elements per patch: {patches.shape[-1]}\")\n\nn = int(np.sqrt(patches.shape[1]))\nplt.figure(figsize=(4, 4))\nfor i, patch in enumerate(patches[0]):\n    ax = plt.subplot(n, n, i + 1)\n    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n    plt.imshow(patch_img.numpy()/255)\n    plt.axis(\"off\")","metadata":{"id":"IgWmn1qitGCc","outputId":"0b7a8865-5c8d-4912-c74d-bc9c3f70a9fd","execution":{"iopub.status.busy":"2022-08-07T10:50:08.272097Z","iopub.execute_input":"2022-08-07T10:50:08.273318Z","iopub.status.idle":"2022-08-07T10:50:10.421970Z","shell.execute_reply.started":"2022-08-07T10:50:08.273278Z","shell.execute_reply":"2022-08-07T10:50:10.420959Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Patch encoding\n\nclass PatchEncoder(layers.Layer):\n    def __init__(self, num_patches, projection_dim):\n        super(PatchEncoder, self).__init__()\n        self.num_patches = num_patches\n        self.projection = layers.Dense(units=projection_dim)\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n\n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded","metadata":{"id":"TBxeuhl3t5bl","execution":{"iopub.status.busy":"2022-08-07T10:50:14.555795Z","iopub.execute_input":"2022-08-07T10:50:14.556901Z","iopub.status.idle":"2022-08-07T10:50:14.565771Z","shell.execute_reply.started":"2022-08-07T10:50:14.556857Z","shell.execute_reply":"2022-08-07T10:50:14.564719Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def create_vit_classifier():\n    inputs = layers.Input(shape=input_shape)\n    # Augment data.\n    augmented = data_augmentation(inputs)\n    # Create patches.\n    patches = Patches(patch_size)(augmented)\n    # Encode patches.\n    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n\n    # Create multiple layers of the Transformer block.\n    for _ in range(transformer_layers):\n        # Layer normalization 1.\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n        # Create a multi-head attention layer.\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n        )(x1, x1)\n        # Skip connection 1.\n        x2 = layers.Add()([attention_output, encoded_patches])\n        # Layer normalization 2.\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        # MLP.\n        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n        # Skip connection 2.\n        encoded_patches = layers.Add()([x3, x2])\n\n    # Create a [batch_size, projection_dim] tensor.\n    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    representation = layers.Flatten()(representation)\n    representation = layers.Dropout(0.5)(representation)\n    # Add MLP.\n    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n    # Classify outputs.\n    logits = layers.Dense(num_classes)(features)\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=logits)\n    return model","metadata":{"id":"UThifznBt_Nk","execution":{"iopub.status.busy":"2022-08-07T10:50:26.594340Z","iopub.execute_input":"2022-08-07T10:50:26.595023Z","iopub.status.idle":"2022-08-07T10:50:26.604073Z","shell.execute_reply.started":"2022-08-07T10:50:26.594986Z","shell.execute_reply":"2022-08-07T10:50:26.603109Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def run_experiment(model):\n    optimizer = tfa.optimizers.AdamW(\n        learning_rate=learning_rate, weight_decay=weight_decay\n    )\n\n    model.compile(\n    optimizer=optimizer,\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n    )\n\n    history = model.fit(\n        x_train, y_train_enc,\n        batch_size=batch_size,\n        epochs=num_epochs,\n        validation_data=(x_validation, y_validation_enc),\n    )\n\n    return history, model\n\n\nvit_classifier = create_vit_classifier()\nhistory, model = run_experiment(vit_classifier)","metadata":{"id":"UTzxqKbLuPqt","outputId":"f2a049ad-26ea-4358-b336-376274214429","execution":{"iopub.status.busy":"2022-08-07T10:57:31.175645Z","iopub.execute_input":"2022-08-07T10:57:31.176004Z","iopub.status.idle":"2022-08-07T11:00:02.537396Z","shell.execute_reply.started":"2022-08-07T10:57:31.175973Z","shell.execute_reply":"2022-08-07T11:00:02.536332Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"Let's plot the training and validation loss and Accuracy","metadata":{"id":"haLQ-f6Ghir6"}},{"cell_type":"code","source":"def plot_accuracy_and_loss(history):\n    acc = history.history['sparse_categorical_accuracy']\n    val_acc = history.history['val_sparse_categorical_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(acc) + 1)\n\n    plt.plot(epochs, acc, '--r', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.show()\n\n    plt.plot(epochs, loss, '--r', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    plt.show()\n\nplot_accuracy_and_loss(history)","metadata":{"id":"GSLSBpV2ASRb","execution":{"iopub.status.busy":"2022-08-07T11:00:02.539930Z","iopub.execute_input":"2022-08-07T11:00:02.540411Z","iopub.status.idle":"2022-08-07T11:00:02.939917Z","shell.execute_reply.started":"2022-08-07T11:00:02.540373Z","shell.execute_reply":"2022-08-07T11:00:02.938958Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"Let's check predictions by the model on test set which is unlabelled data","metadata":{"id":"RcnuZALvhs0N"}},{"cell_type":"code","source":"import numpy as np\nprediction = model.predict(x_test)\n\n\nfig, ax = plt.subplots(10, 5, figsize=(25,25))\nfig.tight_layout(h_pad=4)\nidx = 0\nfor i in range(0, 10):\n    for j in range(0, 5):\n\n        prediction_label = np.argmax(prediction[idx])\n        ax[i][j].title.set_text('Prediction-{0}({1})'.format(prediction_label, Number(prediction_label).name))\n        image = x_test[idx]\n        ax[i][j].imshow(image)\n        ax[i][j].axis(\"off\")\n        idx += 1 \nplt.show()","metadata":{"id":"ERMwQAc0Fnn5","execution":{"iopub.status.busy":"2022-08-07T10:54:41.175513Z","iopub.execute_input":"2022-08-07T10:54:41.175898Z","iopub.status.idle":"2022-08-07T10:54:45.227502Z","shell.execute_reply.started":"2022-08-07T10:54:41.175866Z","shell.execute_reply":"2022-08-07T10:54:45.226490Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Evaluating the model on training set to see how the model learnt","metadata":{"id":"IUaV1Q2Rh7t4"}},{"cell_type":"code","source":"train_loss, train_acc = model.evaluate(x_train, y_train_enc)\nprint('Accuracy:', train_acc)\nprint('Loss: ', train_loss)","metadata":{"id":"_Ixxs4XKbQ5H","execution":{"iopub.status.busy":"2022-08-07T11:01:31.640147Z","iopub.execute_input":"2022-08-07T11:01:31.640525Z","iopub.status.idle":"2022-08-07T11:01:33.752423Z","shell.execute_reply.started":"2022-08-07T11:01:31.640494Z","shell.execute_reply":"2022-08-07T11:01:33.751454Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"looking at the confusion matrix where if model could learn to differtiate between all the categories","metadata":{"id":"-rkxGdCSiD97"}},{"cell_type":"code","source":"prediction = model.predict(x_train)\ny_pred = [np.argmax(p) for p in prediction]\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\ncm = confusion_matrix(y_pred, y_train_enc)\n\ncm_df = pd.DataFrame(cm,\n                     index = ['dot_line','hbar','line', 'pie', 'vbar'], \n                     columns = ['dot_line','hbar','line', 'pie', 'vbar'])\n\n#Plotting the confusion matrix\nplt.figure(figsize=(10,8))\nsns.heatmap(cm_df, annot=True, fmt='g')\nplt.title('Confusion Matrix on Training Data')\nplt.ylabel('Actal Values')\nplt.xlabel('Predicted Values')\nplt.show()","metadata":{"id":"rhZ7wsCbbOu3","execution":{"iopub.status.busy":"2022-08-07T10:56:50.691236Z","iopub.execute_input":"2022-08-07T10:56:50.691595Z","iopub.status.idle":"2022-08-07T10:56:53.065902Z","shell.execute_reply.started":"2022-08-07T10:56:50.691566Z","shell.execute_reply":"2022-08-07T10:56:53.064967Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Evaluating the model on validation set which is labelled","metadata":{"id":"q6vBnDJ6iP1-"}},{"cell_type":"code","source":"val_loss, val_acc = model.evaluate(x_validation, y_validation_enc)\nprint('Accuracy:', val_acc)\nprint('Loss: ', val_loss)","metadata":{"id":"t1iyXe6sbasq","execution":{"iopub.status.busy":"2022-08-07T11:01:42.760570Z","iopub.execute_input":"2022-08-07T11:01:42.760931Z","iopub.status.idle":"2022-08-07T11:01:43.445401Z","shell.execute_reply.started":"2022-08-07T11:01:42.760899Z","shell.execute_reply":"2022-08-07T11:01:43.444450Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"Checking how the confusion matrix looks like for validation data","metadata":{"id":"j1X0cxtZiVRF"}},{"cell_type":"code","source":"prediction = model.predict(x_validation)\ny_pred = [np.argmax(p) for p in prediction]\ncm = confusion_matrix(y_pred, y_validation_enc)\n\ncm_df = pd.DataFrame(cm,\n                     index = ['dot_line','hbar','line', 'pie', 'vbar'], \n                     columns = ['dot_line','hbar','line', 'pie', 'vbar'])\n\n#Plotting the confusion matrix\nplt.figure(figsize=(10,8))\nsns.heatmap(cm_df, annot=True, fmt='g')\nplt.title('Confusion Matrix on Validation Data')\nplt.ylabel('Actal Values')\nplt.xlabel('Predicted Values')\nplt.show()","metadata":{"id":"Ln3qT8AUcmSy","execution":{"iopub.status.busy":"2022-08-07T11:01:47.647281Z","iopub.execute_input":"2022-08-07T11:01:47.647644Z","iopub.status.idle":"2022-08-07T11:01:49.269998Z","shell.execute_reply.started":"2022-08-07T11:01:47.647614Z","shell.execute_reply":"2022-08-07T11:01:49.268980Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}